{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict \n",
    "from statistics import mean, stdev\n",
    "import pandas as pd\n",
    "from math import pi, sqrt, exp, log\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocess function\n",
    "\n",
    "# This function should prepare the data by reading it from a file and converting it into a useful format for training and testing\n",
    "# The instances where all the attributes are missing are removed, and mean the mean of the attributes in each class is imputed.\n",
    "\n",
    "def preprocess(dataname):\n",
    "    \n",
    "    attribute_list = [\"pose\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x7\",\"x8\",\"x9\",\"x10\",\"x11\",\"y1\",\"y2\",\"y3\",\"y4\",\"y5\",\"y6\",\"y7\",\"y8\",\"y9\",\"y10\",\"y11\"]\n",
    "    df = pd.read_csv(dataname, names = attribute_list)\n",
    "    \n",
    "    # replace all 9999 by nan\n",
    "    df.replace(9999, np.nan, inplace=True)\n",
    "\n",
    "    # get rid of fully identifiable instance (row of nan)\n",
    "    df.dropna(subset=df.columns.difference(['pose']), how='all', inplace = True)\n",
    "    \n",
    "    \n",
    "    # Q5 mean imputation code\n",
    "    poses = df.pose.unique()\n",
    "    for attribute in attribute_list[1:]:\n",
    "        df[attribute] = df[attribute].fillna(df.groupby('pose')[attribute].transform('mean'))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train function\n",
    "\n",
    "# This function calculates and returns a dictionary of the mean, standard deviation and size of the instances\n",
    "# which is later used when calculating the posteriors and likelihoods\n",
    "\n",
    "def train(traindata):\n",
    "\n",
    "    df_groups = traindata.groupby(traindata.pose)\n",
    "    \n",
    "    # get lists of the mean, standard deviation and group size of each pose (class)\n",
    "    mean_dict = df_groups.mean().T.to_dict()\n",
    "    std_dict = df_groups.std().T.to_dict()\n",
    "    group_size = df_groups.size().to_dict()\n",
    "    \n",
    "    summary_dict = defaultdict(list)\n",
    "    \n",
    "    for pose in mean_dict:\n",
    "        for loc in mean_dict[pose]:\n",
    "            summary_dict[pose].append([mean_dict[pose][loc],std_dict[pose][loc], group_size[pose]])\n",
    "    \n",
    "    return summary_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates and returns a dictionary of the posterior probability of each class.\n",
    "\n",
    "def posterior_per_class(summary, instance, row_total):\n",
    "    \n",
    "    posteriors = dict()\n",
    "    for pose in summary:\n",
    "        \n",
    "        # get the ratio of the class to the whole number of instances\n",
    "        posterior = log(summary[pose][0][2]/row_total)\n",
    "        for i in range(len(instance)-1):\n",
    "            \n",
    "            # add the logs of the posteriors to the dictionary of posteriors\n",
    "            if(gaussian_probability(instance[i], summary[pose][i][0], summary[pose][i][1]) > 0):\n",
    "                posterior += log(gaussian_probability(instance[i], summary[pose][i][0], summary[pose][i][1]))\n",
    "        posteriors[pose] = posterior\n",
    "        posterior = 0\n",
    "        \n",
    "    return posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the gaussian probability\n",
    "\n",
    "def gaussian_probability(x, mean, std):\n",
    "    \n",
    "    return 1/(std*sqrt(2*pi)) * exp(-((x-mean)**2/(2*std**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that makes a list of the predictions for the test data instances based on the parameters obtained from the train data\n",
    "\n",
    "def predict(traindata, testdata):\n",
    "    \n",
    "    # get the total number of instances in the test data\n",
    "    row_total = testdata.shape[0]\n",
    "    \n",
    "    # convert the dataframe into a list of instances\n",
    "    instance_list = testdata.values.tolist()\n",
    "    \n",
    "    predictions = list()\n",
    "    \n",
    "    # Get the summary of the train data to use for the posteriors\n",
    "    trained_summary = train(traindata)\n",
    "    \n",
    "    for instance in instance_list:\n",
    "        posteriors = posterior_per_class(trained_summary, instance[1:], row_total)\n",
    "        \n",
    "        # Get the class with the highest posterior for the instance to use as a prediction\n",
    "        predictions.append(max(posteriors.items(), key=operator.itemgetter(1))[0])\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns a dictionary of the precision and recalls of each pose (class)\n",
    "\n",
    "def precision_and_recall(actual_poses, prediction):\n",
    "    \n",
    "    poses = list()\n",
    "    \n",
    "    # get list of poses\n",
    "    for pose in actual_poses:\n",
    "        if pose not in poses:\n",
    "            poses.append(pose)\n",
    "    \n",
    "    eval_dict = defaultdict(list)\n",
    "    \n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    \n",
    "    for pose in poses:\n",
    "        for i in range(len(actual_poses)):\n",
    "            \n",
    "            # get the TP, FP and FN by comparing each pose in actual poses and the predicted poses\n",
    "            if(pose == actual_poses[i] and pose == prediction[i]):\n",
    "                TP += 1\n",
    "            elif(pose != actual_poses[i] and pose == prediction[i]):\n",
    "                FP += 1\n",
    "            elif(pose == actual_poses[i] and pose != prediction[i]):\n",
    "                FN += 1\n",
    "        eval_dict[pose].append(TP/(TP + FP))\n",
    "        eval_dict[pose].append(TP/(TP + FN))\n",
    "        TP = 0\n",
    "        FP = 0\n",
    "        FN = 0\n",
    "    \n",
    "    return eval_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that computes the correct predictions out of the total number of predictions\n",
    "\n",
    "def accuracy(actual_poses, prediction):\n",
    "    \n",
    "    correct = 0\n",
    "    for i in range(len(actual_poses)):\n",
    "        if (actual_poses[i] == prediction[i]):\n",
    "            correct += 1\n",
    "    \n",
    "    return correct/len(actual_poses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Related to Q1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that computes the macro average of the perforance of the classifier\n",
    "\n",
    "def macro_averaging(eval_dict):\n",
    "    \n",
    "    p_r_sum = [0, 0]\n",
    "    \n",
    "    # Number of classes\n",
    "    no_class = 0\n",
    "    \n",
    "    for pose in eval_dict:\n",
    "        no_class += 1\n",
    "        \n",
    "        # Precision sum\n",
    "        p_r_sum[0] += eval_dict[pose][0]\n",
    "        # Recall sum\n",
    "        p_r_sum[1] += eval_dict[pose][1]\n",
    "    \n",
    "    p_r_mean = [x/no_class for x in p_r_sum]\n",
    "    \n",
    "    return p_r_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Related to Q1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "## related to Q1\n",
    "# Function that computes the macro average of the perforance of the classifier\n",
    "# implements logic from lecture slide 37 of Model Evaluation\n",
    "\n",
    "def micro_averaging(actual_poses, prediction):\n",
    "    \n",
    "    poses = list()\n",
    "    \n",
    "    # get list of poses\n",
    "    for pose in actual_poses:\n",
    "        if pose not in poses:\n",
    "            poses.append(pose)\n",
    "    \n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    \n",
    "    p_r_dict = defaultdict(list)\n",
    "    \n",
    "    for pose in poses:\n",
    "        for i in range(len(actual_poses)):\n",
    "            if(pose == actual_poses[i] and pose == prediction[i]):\n",
    "                TP += 1\n",
    "            elif(pose != actual_poses[i] and pose == prediction[i]):\n",
    "                FP += 1\n",
    "            elif(pose == actual_poses[i] and pose != prediction[i]):\n",
    "                FN += 1\n",
    "        p_r_dict[pose] = [TP, FP, FN]\n",
    "        TP = 0\n",
    "        FP = 0\n",
    "        FN = 0\n",
    "    \n",
    "    TPsum = 0\n",
    "    TPFP = 0\n",
    "    TPFN = 0\n",
    "    \n",
    "    for pose in p_r_dict:\n",
    "        TPsum += p_r_dict[pose][0]\n",
    "        TPFP += p_r_dict[pose][0] + p_r_dict[pose][1]\n",
    "        TPFN += p_r_dict[pose][0] + p_r_dict[pose][2]\n",
    "        \n",
    "    return [TPsum/TPFP, TPsum/TPFN]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Related to Q1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that computes the F1-score of the each evaluation metric calculation\n",
    "\n",
    "def f1score(precision_recall):\n",
    "    \n",
    "    P = precision_recall[0]\n",
    "    R = precision_recall[1]\n",
    "    return ((2*P*R)/(P + R))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that evaluates the performance of the classifier, comparing the actual data and predictions\n",
    "# 'A scoring function'\n",
    "\n",
    "def evaluate(testdata, prediction):\n",
    "    \n",
    "    actual_poses = testdata['pose'].tolist()\n",
    "\n",
    "    print(\"accuracy: \", accuracy(actual_poses, prediction))\n",
    "    \n",
    "    eval_dict = precision_and_recall(actual_poses, prediction)\n",
    "    \n",
    "    macro = macro_averaging(eval_dict)\n",
    "    micro = micro_averaging(actual_poses, prediction)\n",
    "    print(\"Macro-average Precision:\", macro[0], \"Macro-average Recall:\", macro[1], \"F1-score:\", f1score(macro))\n",
    "    print(\"Micro-average Precision:\", micro[0], \"Micro-average Recall:\", micro[1], \"F1-score:\", f1score(micro))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on train (train data used as test data):\n",
      "accuracy:  0.8989071038251366\n",
      "Macro-average Precision: 0.9011067359560316 Macro-average Recall: 0.9016762383725847 F1-score: 0.9013913972108842\n",
      "Micro-average Precision: 0.8989071038251366 Micro-average Recall: 0.8989071038251366 F1-score: 0.8989071038251366\n",
      "\n",
      "\n",
      "test:\n",
      "accuracy:  0.7678571428571429\n",
      "Macro-average Precision: 0.7567136784783843 Macro-average Recall: 0.7523412698412699 F1-score: 0.7545211397459243\n",
      "Micro-average Precision: 0.7678571428571429 Micro-average Recall: 0.7678571428571429 F1-score: 0.7678571428571429\n"
     ]
    }
   ],
   "source": [
    "preprocessed_train = preprocess('train.csv')\n",
    "preprocessed_test = preprocess('test.csv')\n",
    "\n",
    "print('train on train (train data used as test data):')\n",
    "evaluate(preprocessed_train, predict(preprocessed_train, preprocessed_train))\n",
    "print('\\n')\n",
    "print('test:')\n",
    "evaluate(preprocessed_test, predict(preprocessed_train, preprocessed_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1\n",
    "Since this is a multiclass classification problem, there are multiple ways to compute precision, recall, and F-score for this classifier. Implement at least two of the methods from the \"Model Evaluation\" lecture and discuss any differences between them. (The implementation should be your own and should not just call a pre-existing function.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The macro_averaging(), micro_averaging() and f1score() function are all related to this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5\n",
    "Naive Bayes ignores missing values, but in pose recognition tasks the missing values can be informative. Missing values indicate that some part of the body was obscured and sometimes this is relevant to the pose (e.g., holding one hand behind the back). Are missing values useful for this task? Implement a method that incorporates information about missing values and demonstrate whether it changes the classification results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# A mean imputation was additionally performed in the preprocessing function as a way of incorporating information about the missing values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
